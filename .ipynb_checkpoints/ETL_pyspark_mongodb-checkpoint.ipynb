{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c6aa6b-25a1-493e-8bc7-7b3fb42d3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['HADOOP_HOME'] = \"C:\\Hadoop\"\n",
    "sys.path.append(\"C:\\Hadoop\\bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d00ac0-befa-4d8d-834b-9f3429c86f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "builder.\\\n",
    "appName(\"pyspark-notebook2\").\\\n",
    "master(\"local\").\\\n",
    "config(\"spark.executor.memory\", \"1g\").\\\n",
    "config(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/Project6.project\").\\\n",
    "config(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/Project6.project\").\\\n",
    "config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f940a9-a2d8-49fc-a3db-6b5fc3424336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- acronym: string (nullable = true)\n",
      " |-- contentUpdateDate: timestamp (nullable = true)\n",
      " |-- ecMaxContribution: string (nullable = true)\n",
      " |-- ecSignatureDate: timestamp (nullable = true)\n",
      " |-- endDate: timestamp (nullable = true)\n",
      " |-- frameworkProgramme: string (nullable = true)\n",
      " |-- fundingScheme: string (nullable = true)\n",
      " |-- grantDoi: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- legalBasis: string (nullable = true)\n",
      " |-- masterCall: string (nullable = true)\n",
      " |-- nature: void (nullable = true)\n",
      " |-- objective: string (nullable = true)\n",
      " |-- rcn: integer (nullable = true)\n",
      " |-- startDate: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- subCall: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- topics: string (nullable = true)\n",
      " |-- totalCost: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbcf138e-71b6-4867-a46f-739750e88cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- acronym: string (nullable = true)\n",
      " |-- contentUpdateDate: timestamp (nullable = true)\n",
      " |-- ecMaxContribution: string (nullable = true)\n",
      " |-- ecSignatureDate: timestamp (nullable = true)\n",
      " |-- endDate: timestamp (nullable = true)\n",
      " |-- frameworkProgramme: string (nullable = true)\n",
      " |-- fundingScheme: string (nullable = true)\n",
      " |-- grantDoi: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- legalBasis: string (nullable = true)\n",
      " |-- masterCall: string (nullable = true)\n",
      " |-- nature: void (nullable = true)\n",
      " |-- objective: string (nullable = true)\n",
      " |-- rcn: integer (nullable = true)\n",
      " |-- startDate: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- subCall: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- topics: string (nullable = true)\n",
      " |-- totalCost: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read\\\n",
    "    .option(\"sep\", \";\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .format(\"mongo\").load()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc47e5ef-83f9-4280-beb5-d3164818704b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+-----------------+-------------------+-------------------+------------------+-------------+-----------------+---------+-----------+--------------------+------+--------------------+------+-------------------+------+--------------------+--------------------+--------------------+-----------+\n",
      "|                 _id|    acronym|  contentUpdateDate|ecMaxContribution|    ecSignatureDate|            endDate|frameworkProgramme|fundingScheme|         grantDoi|       id| legalBasis|          masterCall|nature|           objective|   rcn|          startDate|status|             subCall|               title|              topics|  totalCost|\n",
      "+--------------------+-----------+-------------------+-----------------+-------------------+-------------------+------------------+-------------+-----------------+---------+-----------+--------------------+------+--------------------+------+-------------------+------+--------------------+--------------------+--------------------+-----------+\n",
      "|{63cfea2a40978a13...|   PROGRESS|2022-09-28 13:21:14|          2657500|2022-09-21 01:00:00|2027-12-31 00:00:00|           HORIZON|          ERC|10.3030/101043356|101043356|HORIZON.1.1|        ERC-2021-COG|  null|Quantitative prov...|242239|2023-01-01 00:00:00|SIGNED|        ERC-2021-COG|Reading provenanc...|        ERC-2021-COG|    2657500|\n",
      "|{63cfea2a40978a13...|      SCOPE|2022-10-06 18:58:49|           150000|2022-09-29 01:00:00|2024-02-29 00:00:00|           HORIZON|      ERC-POC|10.3030/101100999|101100999|HORIZON.1.1|       ERC-2022-POC2|  null|Monitoring of hum...|242302|2022-09-01 01:00:00|SIGNED|       ERC-2022-POC2|CRISPR Point-of-C...|       ERC-2022-POC2|          0|\n",
      "|{63cfea2a40978a13...|   e-IRGSP7|2022-09-04 14:39:32|           302259|2022-03-28 01:00:00|2023-09-30 01:00:00|           HORIZON|          CSA|10.3030/101057802|101057802|HORIZON.1.3|HORIZON-INFRA-202...|  null|e-IRGSP7 will pro...|241435|2022-04-01 01:00:00|SIGNED|HORIZON-INFRA-202...|e-Infrastructure ...|HORIZON-INFRA-202...|     302260|\n",
      "|{63cfea2a40978a13...|   EOinTime|2022-09-27 18:17:17|          1735300|2022-09-20 01:00:00|2024-03-31 00:00:00|           HORIZON|   HORIZON-AG|10.3030/190100375|190100375|HORIZON.3.1|HORIZON-EIC-2022-...|  null|LiveEO will offer...|242179|2022-10-01 01:00:00|SIGNED|HORIZON-EIC-2022-...|Satellite-based c...|HORIZON-EIC-2022-...|    2489000|\n",
      "|{63cfea2a40978a13...|BosomShield|2022-09-12 17:42:49|        2595355,2|2022-07-06 01:00:00|2026-08-31 01:00:00|           HORIZON|HORIZON-AG-UN|10.3030/101073222|101073222|HORIZON.1.2|HORIZON-MSCA-2021...|  null|Breast cancer (BC...|241045|2022-09-01 01:00:00|SIGNED|HORIZON-MSCA-2021...|A comprehensive C...|HORIZON-MSCA-2021...|          0|\n",
      "|{63cfea2a40978a13...|SECRET-DOCK|2022-08-26 16:47:47|           150000|2022-07-08 01:00:00|2024-02-29 00:00:00|           HORIZON|      ERC-POC|10.3030/101082277|101082277|HORIZON.1.1|       ERC-2022-POC2|  null|Stress is the for...|240359|2022-09-01 01:00:00|SIGNED|       ERC-2022-POC2|Transient inactiv...|       ERC-2022-POC2|          0|\n",
      "|{63cfea2a40978a13...|      FLORA|2022-09-27 18:20:12|          1495428|2022-09-21 01:00:00|2027-09-30 01:00:00|           HORIZON|          ERC|10.3030/101039402|101039402|HORIZON.1.1|        ERC-2021-STG|  null|Food systems are ...|242241|2022-10-01 01:00:00|SIGNED|        ERC-2021-STG|Sustainable and h...|        ERC-2021-STG|    1495428|\n",
      "|{63cfea2a40978a13...|      UCoCo|2022-09-05 09:18:49|        173847,36|2022-08-08 01:00:00|2024-12-31 00:00:00|           HORIZON|      MSCA-PF|10.3030/101060427|101060427|HORIZON.1.2|HORIZON-MSCA-2021...|  null|Layered two-dimen...|241209|2023-01-01 00:00:00|SIGNED|HORIZON-MSCA-2021...|Ultrafast Control...|HORIZON-MSCA-2021...|          0|\n",
      "|{63cfea2a40978a13...|      SiGNE|2022-06-24 15:33:09|        7979282,5|2022-06-10 01:00:00|2026-08-31 01:00:00|           HORIZON|          RIA|10.3030/101069738|101069738|HORIZON.2.5|HORIZON-CL5-2021-...|  null|SiGNE will delive...|237941|2022-09-01 01:00:00|SIGNED|HORIZON-CL5-2021-...|Composite Silicon...|HORIZON-CL5-2021-...|  7979282,5|\n",
      "|{63cfea2a40978a13...|RADIOBLOCKS|2022-12-11 19:03:54|          8903952|2022-12-01 00:00:00|2027-02-28 00:00:00|           HORIZON|          RIA|10.3030/101093934|101093934|HORIZON.1.3|HORIZON-INFRA-202...|  null|The goal of the R...|243449|2023-03-01 00:00:00|SIGNED|HORIZON-INFRA-202...|New science in Ra...|HORIZON-INFRA-202...| 8903952,25|\n",
      "|{63cfea2a40978a13...|    NEMECYS|2022-12-16 14:30:16|          4208360|2022-12-08 00:00:00|2025-12-31 00:00:00|           HORIZON|          RIA|10.3030/101094323|101094323|HORIZON.2.1|HORIZON-HLTH-2022...|  null|The European heal...|243798|2023-01-01 00:00:00|SIGNED|HORIZON-HLTH-2022...|NEw MEdical CYber...|HORIZON-HLTH-2022...|    4208360|\n",
      "|{63cfea2a40978a13...|     PHENET|2022-12-09 15:36:53|          9993469|2022-12-01 00:00:00|2027-12-31 00:00:00|           HORIZON|          RIA|10.3030/101094587|101094587|HORIZON.1.3|HORIZON-INFRA-202...|  null|Europe urgently n...|243511|2023-01-01 00:00:00|SIGNED|HORIZON-INFRA-202...|Tools and methods...|HORIZON-INFRA-202...|10237131,25|\n",
      "|{63cfea2a40978a13...|   HOMEMADE|2022-12-23 15:18:09|       2113168,75|2022-12-18 00:00:00|2025-12-31 00:00:00|           HORIZON|          RIA|10.3030/101081785|101081785|HORIZON.2.4|HORIZON-CL4-2021-...|  null|The HOMEMADE proj...|243966|2023-01-01 00:00:00|SIGNED|HORIZON-CL4-2021-...|High Q factOr Min...|HORIZON-CL4-2021-...|    2113170|\n",
      "|{63cfea2a40978a13...|    CAPONEU|2022-12-23 15:14:31|          1864789|2022-12-14 00:00:00|2027-01-31 00:00:00|           HORIZON|          RIA|10.3030/101094658|101094658|HORIZON.2.2|HORIZON-CL2-2022-...|  null|The project sets ...|243924|2023-02-01 00:00:00|SIGNED|HORIZON-CL2-2022-...|The Cartography o...|HORIZON-CL2-2022-...|    1866039|\n",
      "|{63cfea2a40978a13...|    DISCERN|2022-12-09 15:40:47|          8857813|2022-12-06 00:00:00|2027-12-31 00:00:00|           HORIZON|          RIA|10.3030/101096888|101096888|HORIZON.2.1|HORIZON-MISS-2021...|  null|\"The overall goal...|243539|2023-01-01 00:00:00|SIGNED|HORIZON-MISS-2021...|Discovering the c...|HORIZON-MISS-2021...|    8857814|\n",
      "|{63cfea2a40978a13...|    FutuRes|2022-12-23 15:14:54|          2740550|2022-12-13 00:00:00|2026-01-31 00:00:00|           HORIZON|          RIA|10.3030/101094741|101094741|HORIZON.2.2|HORIZON-CL2-2022-...|  null|The dynamics of t...|243922|2023-02-01 00:00:00|SIGNED|HORIZON-CL2-2022-...|Towards a Resilie...|HORIZON-CL2-2022-...|    2740550|\n",
      "|{63cfea2a40978a13...|       AIDA|2022-12-09 15:36:46|       6334803,75|2022-12-05 00:00:00|2026-12-31 00:00:00|           HORIZON|          RIA|10.3030/101095359|101095359|HORIZON.2.1|HORIZON-HLTH-2022...|  null|Most cases of gas...|243508|2023-01-01 00:00:00|SIGNED|HORIZON-HLTH-2022...|An Artificially I...|HORIZON-HLTH-2022...| 6334803,75|\n",
      "|{63cfea2a40978a13...|    gEneSys|2022-12-16 14:30:16|       2656488,75|2022-12-13 00:00:00|2026-01-31 00:00:00|           HORIZON|          RIA|10.3030/101094326|101094326|HORIZON.2.2|HORIZON-CL2-2022-...|  null|gEneSys advances ...|243797|2023-02-01 00:00:00|SIGNED|HORIZON-CL2-2022-...|Transforming Gend...|HORIZON-CL2-2022-...| 2656488,75|\n",
      "|{63cfea2a40978a13...|   DEMOCRAT|2022-12-16 14:28:08|       2951909,75|2022-12-09 00:00:00|2026-02-28 00:00:00|           HORIZON|          RIA|10.3030/101095106|101095106|HORIZON.2.2|HORIZON-CL2-2022-...|  null|EU-society faces ...|243774|2023-03-01 00:00:00|SIGNED|HORIZON-CL2-2022-...|Education for Res...|HORIZON-CL2-2022-...|    2951910|\n",
      "|{63cfea2a40978a13...|   AI4TRUST|2022-12-09 15:34:27|        5950682,5|2022-12-06 00:00:00|2026-02-28 00:00:00|           HORIZON|          RIA|10.3030/101070190|101070190|HORIZON.2.4|HORIZON-CL4-2021-...|  null|Increasing eviden...|243486|2023-01-01 00:00:00|SIGNED|HORIZON-CL4-2021-...|AI-based-technolo...|HORIZON-CL4-2021-...|  5950682,5|\n",
      "+--------------------+-----------+-------------------+-----------------+-------------------+-------------------+------------------+-------------+-----------------+---------+-----------+--------------------+------+--------------------+------+-------------------+------+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e37c8d-0bf0-407c-bd16-b360a07a8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working!!\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_OCDE_ind = SparkSession.\\\n",
    "builder.\\\n",
    "appName(\"pyspark-notebook2\").\\\n",
    "master(\"local\").\\\n",
    "config(\"spark.executor.memory\", \"1g\").\\\n",
    "config(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/Project6.OCDE_ind\").\\\n",
    "config(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/Project6.OCDE_ind\").\\\n",
    "config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7922a2d-b39b-4a76-a2db-5d318f3d1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_OCDE_ind.conf.set('spark.sql.caseSensitive', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73c0e31-4c87-4731-b25b-03efd5f4d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Flag Codes: string (nullable = true)\n",
      " |-- Flags: string (nullable = true)\n",
      " |-- IND: string (nullable = true)\n",
      " |-- Industry: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- PowerCode: string (nullable = true)\n",
      " |-- PowerCode Code: string (nullable = true)\n",
      " |-- Reference Period: string (nullable = true)\n",
      " |-- Reference Period Code: string (nullable = true)\n",
      " |-- TIME: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Unit: string (nullable = true)\n",
      " |-- Unit Code: string (nullable = true)\n",
      " |-- VAR: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Variable: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_OCDE_ind = spark_OCDE_ind.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "df_OCDE_ind.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131f8291-1af4-4826-aad8-3ec6f9d92fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+-----+--------+--------+---------+--------------+----------------+---------------------+----+----+----+---------+----+-----+--------+-----+\n",
      "|Country|Flag Codes|Flags|  IND|Industry|LOCATION|PowerCode|PowerCode Code|Reference Period|Reference Period Code|TIME|Time|Unit|Unit Code| VAR|Value|Variable|  _id|\n",
      "+-------+----------+-----+-----+--------+--------+---------+--------------+----------------+---------------------+----+----+----+---------+----+-----+--------+-----+\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2010|2010|Euro|      EUR|PROD|48...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2011|2011|Euro|      EUR|PROD|51...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2012|2012|Euro|      EUR|PROD|51...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2013|2013|Euro|      EUR|PROD|52...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2014|2014|Euro|      EUR|PROD|54...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2015|2015|Euro|      EUR|PROD|55...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2016|2016|Euro|      EUR|PROD|57...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2017|2017|Euro|      EUR|PROD|60...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2018|2018|Euro|      EUR|PROD|62...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            null|                 null|2019|2019|Euro|      EUR|PROD|63...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2010|2010|Euro|      EUR|PRDK|51...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2011|2011|Euro|      EUR|PRDK|53...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2012|2012|Euro|      EUR|PRDK|53...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2013|2013|Euro|      EUR|PRDK|53...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2014|2014|Euro|      EUR|PRDK|54...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2015|2015|Euro|      EUR|PRDK|55...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2016|2016|Euro|      EUR|PRDK|57...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2017|2017|Euro|      EUR|PRDK|58...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2018|2018|Euro|      EUR|PRDK|59...|   Pr...|{6...|\n",
      "|  Ge...|      null| null|D0...|   TOTAL|     DEU|    Mi...|             6|            2015|                 2015|2019|2019|Euro|      EUR|PRDK|59...|   Pr...|{6...|\n",
      "+-------+----------+-----+-----+--------+--------+---------+--------------+----------------+---------------------+----+----+----+---------+----+-----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_OCDE_ind.show(truncate=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "384d8bd0-afa4-46c4-b3cd-32b5d20033e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|TIME|sum                  |\n",
      "+----+---------------------+\n",
      "|2019|6.788903849673239E10 |\n",
      "|2018|2.1291300724014713E11|\n",
      "|2017|3.94469323394116E11  |\n",
      "|2016|3.7594543831080566E11|\n",
      "|2015|3.6412414894067523E11|\n",
      "|2014|3.5111582626191614E11|\n",
      "|2013|3.402527159873131E11 |\n",
      "|2012|3.293067033760879E11 |\n",
      "|2011|3.1890672745869025E11|\n",
      "|2010|2.9633225599564276E11|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get some basic information\n",
    "# production by year worldwide\n",
    "\n",
    "from pyspark.sql.functions import sum,avg,max,count, round, col, asc\n",
    "\n",
    "\n",
    "df_time_sum = (df_OCDE_ind.groupby('TIME')\\\n",
    ".agg(sum('Value')\\\n",
    ".alias('sum'))\\\n",
    ".sort(col('TIME')\\\n",
    ".desc())\\\n",
    ".show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65bcde7a-d204-4f86-b82b-61e6ca0489c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|TIME|                 sum|\n",
      "+----+--------------------+\n",
      "|2019|6.788903849673236E10|\n",
      "|2018|2.129130072401472...|\n",
      "|2017|3.944693233941148E11|\n",
      "|2016|3.759454383108054E11|\n",
      "|2015|3.641241489406749E11|\n",
      "|2014|3.511158262619164E11|\n",
      "|2013|3.402527159873139E11|\n",
      "|2012|3.293067033760883E11|\n",
      "|2011|3.189067274586900...|\n",
      "|2010|2.963322559956424...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time_sum = (df_OCDE_ind.groupby('TIME')\\\n",
    ".agg(sum('Value')\\\n",
    ".alias('sum'))\\\n",
    ".sort(col('TIME')\\\n",
    ".desc()))\n",
    "\n",
    "df_time_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec64fc-2c17-4ec9-b5c2-e616e569d11e",
   "metadata": {},
   "source": [
    "LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c014010-98b2-415c-ad00-7718b113bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-8.0.32-cp310-cp310-win_amd64.whl (7.9 MB)\n",
      "     ---------------------------------------- 7.9/7.9 MB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<=3.20.3,>=3.11.0 in c:\\users\\marta\\anaconda3\\envs\\data\\lib\\site-packages (from mysql-connector-python) (3.20.3)\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-8.0.32\n"
     ]
    }
   ],
   "source": [
    "# Write to MySQL Table\n",
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f519b36-aa5a-46c7-9933-490a594638b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e90f807-8310-4ce4-9fd0-49a941c3f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying woth sql\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_OCDE_ind = SparkSession.\\\n",
    "builder.\\\n",
    "appName(\"pyspark-notebook2\").\\\n",
    "master(\"local\").\\\n",
    "config(\"spark.executor.memory\", \"1g\").\\\n",
    "config(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/Project6.OCDE_ind\").\\\n",
    "config(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/Project6.OCDE_ind\").\\\n",
    "config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "config(\"spark.driver.extraClassPath\", \"C:\\Spark\\spark-3.3.1-bin-hadoop3\\jars\\mysql-connector-j-8.0.32.jar\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99415e1e-0001-4f13-846a-e6c25e812822",
   "metadata": {},
   "outputs": [],
   "source": [
    "config(\"spark.jars\", \"mysql-connector-java-5.1.45-bin.jar\").\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3f9cc76f-47d1-46a9-a29e-51e756ae3500",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o343.save.\n: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28280\\3273887775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"root\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"password\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"M1a2r3t4a5!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m   \u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o343.save.\n: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "df_time_sum.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:mysql://localhost:3306/Project6\") \\\n",
    "  .option(\"dbtable\", \"test1\") \\\n",
    "  .option(\"user\", \"root\") \\\n",
    "  .option(\"password\", \"M1a2r3t4a5!\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec3f8d-ed9a-4575-ae75-20e3a23ac57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ac1c76f-9295-49bf-87d2-7d35fca59f4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o352.save.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28280\\1049562828.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mdbtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0muser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'root'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m           password='M1a2r3t4a5!').mode('append').save()\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o352.save.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:229)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "df_time_sum.write.format('jdbc').options(\n",
    "          url='jdbc:mysql://localhost/Project6',\n",
    "          driver='com.mysql.jdbc.Driver',\n",
    "          dbtable='test1',\n",
    "          user='root',\n",
    "          password='M1a2r3t4a5!').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c687b1e-a94e-4331-8c27-5bb97191b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin_time = datetime.datetime.now()\n",
    "# end_time = datetime.datetime.now()\n",
    "# print(f\"Script started at: {begin_time}\")\n",
    "# print(f\"Script completed at: {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59552426-c8c2-43f5-9931-91f4f2539e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.1.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c24df-73c2-4db7-be02-296a62dcce87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
